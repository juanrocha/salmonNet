---
title: "How far does a shock event spreads on a network? Detecting causality and spreading on the salmon trade network"
author: "Juan Rocha, Jessica Gephart"
address:
  - code: a
    address: Stockholm Resilience Centre, Stocholm University, Kräftriket 2B, 10691 Stockholm
  - code: b
    address: Beijer Institute, Swedish Royal Academy of Sciences, Lilla Frescativägen 4A, 104 05 Stockholm
date: "10/5/2017"
output:
  html_document:
    toc: yes
    deep: 2
    toc_float:
      collapsed: true
      smooth_scroll: true
    code_folding: hide 
  word_document: null
  pdf_document:
    citation_package: natbib
    toc: no
    toc_depth: 2
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

## clean environment
rm(list = ls())

## load required libraries
library(network)
library(sna)
library(tidyverse)
library(GGally)
library(rEDM)
library(gridBase)
library(grid)
library(gridExtra)

```

## Introduction

Hi Jessica - This short report intents to replicate what I've done so far with the salmon data you gave me. I'm also using it as a sketch of the abstract to submit to CompleNet. It has to be a 2 pages extended abstract with a figure and references. But first to what I've done. First load the data:

```{r data}
dat <- as_tibble(
    read.csv(
    "~/Documents/Projects/Salmon_Jessica/data/salmon_agg_clean_data.csv"))
```
Currently I'm using my local version of your file, you just have to change the path to run it on your machine. The next step was some data cleaning, I got rid of the NES areas and few importers that do not have ISO codes and were creating problematic missing values down the road. I also transformed your `Agg.Weight` variable to logarithmic scale in order to get a more normal distribution of the trade flows.

```{r data_cleaning}
# make the period factor so you can facet
dat <- mutate(dat,
    fac_period = factor(Period),
    log_weight = log1p(Agg.Weight))

# get rid of weird reported areas
dat <- filter(dat, Exporter != "Areas, nes" , Exporter != "Other Asia, nes" , Exporter != "Other Europe, nes" , Importer != "Areas, nes" , Importer != "Other Asia, nes" , Importer != "Other Europe, nes" , Importer != "Other Africa, nes") %>% droplevels()

# there is a problem with missing factors
dat$Importer.ISO3c <- forcats::fct_explicit_na(dat$Importer.ISO3c, na_level = "missing")
## this is the list of extra problematic factors on importer side
dat %>% filter(Importer.ISO3c == "missing") %>% select(Importer) %>% pull() %>% droplevels() %>% levels()

dat <- filter(dat,  Importer != "Br. Virgin Isds" , Importer != "Bunkers" , Importer != "Curaçao" , Importer != "Saint-Barthélemy", Importer != "Special Categories" ) %>% droplevels()
```

With that cleaning done you can visualize the adjacency matrix of trade flow over time. It helped me decide whether using multiple networks, or assume that the network is amost static and what change is just the flow represented by the links. The difference is that in the dynamic network framework, links appear and dissapear as time pass. With a static network, the link exist if there is normally trade in that chanel but takes value zero every now and then if there is no trade. 

```{r}
# adjacency matrix over time
g <- ggplot(
    data = dat %>%
    select(Importer, Exporter, Agg.Weight, Period, log_weight),
    aes(Importer, Exporter)
    ) + geom_raster(aes(fill = log_weight)) + facet_wrap(~Period) +
    theme_light(base_size = 8) +
    theme(axis.text.x =element_blank(),
        axis.text.y = element_blank())
g
```
I will come back to the network representation later. For now is good to note that the adjacency matrix does not change much over time, so a static network is a reasonable assumption. Only the last 2-3 months have less data but that can be due to lags on the data report of Comtrade rather than inexistence of links.

## Cascading effects: is the import of a country affected by the changes on exports of another trading partner?

The key point of this exercise is to identify whether an event somewhere leave a statistical signature as to change the trade patterns somewhere else. We focused first on the example of Chilean red tides that decrease salmon exports from Chile, but we don't know what countries (and their stocks) cover up for Chilean decline and how far on the trade network a shock event could spread. More formally what we are testing is if the trade flow of two countries $A$ and $B$ is affected by the trade flow of countries $A$ and $C$. If that's the case, the time series of the link A-B should contain information of the timeseries of the link A-C. Following that rationale I applied convergent cross mapping techniques to detect causality between two time series that invole trade flows between three countries. Let's get to the minimal example of Chile.

```{r pressure, echo=FALSE}
## this first step is a waste of time, I do the same later below and accidentaly duplicate entries. For now I leave it this way because then I can explore first the link between chile and usa and then I can do it for all links incoming to usa.
df1 <- filter(dat, Exporter.ISO3c == "CHL", Importer.ISO3c == "USA") %>% select(Period, Exporter = Exporter.ISO3c, Importer = Importer.ISO3c,  log_weight)

time_points <- dat %>% select(Period) %>% unique() %>%
  arrange(Period) %>%
  mutate(time = row_number())

df1 <- bind_rows(df1,
  filter(dat, Importer.ISO3c == "USA")  %>%
  select(Period, Exporter = Exporter.ISO3c, Importer = Importer.ISO3c, log_weight)) %>%
  unite(link, Exporter, Importer, remove = TRUE) %>%
  left_join(time_points)

df1 <- df1 %>%
  group_by(link) %>% unique() %>%
  mutate(norm_weight = (log_weight - mean(log_weight, na.rm = TRUE)) / sd(log_weight, na.rm = TRUE),
    n_obs = n())
```

The code above create a smaller dataset that focus on the trade from Chile to USA, and later expands to all salmon imports to USA. I found two main red tide events impacting Chile in February and May 2016. Then I plot the time series of each link to USA marking with a red line the red tide events.

```{r}
ggplot(data = df1 %>% filter (n_obs > 25),
  aes(x = time, y = exp(log_weight)  )) + geom_line(aes(color = link)) +
  geom_vline(xintercept = c(22,25), color = 'red') + 
    labs( x = "time [months]", y = "Tons of salmon related commodities") +
  facet_wrap(~link)
```
One can see that there is indeed a deep on the trade flow from Chile to USA, shortly followed by an increase in imports from the Netherlands after the first event, and another increase in trade with Canada after the second event. Note that in the salmon trade data only for the USA there is `r length(levels(as.factor(df1$link)))` different links. Here I'm reducing the links to time series where we have at least 25 observations (out of the `r max(df1$time)` months available), and for future analysis I will drop the last two months of the dataset that seem incomplete. Also note that the first time I ran the CCM analysis it didn't work because I didn't normalize the data. Now I'm normalizing it by demean it and divided by the standard deviation, making sure all time series have mean = 0 and variance = 1.

```{r}
ggplot(data = df1 %>% filter(n_obs > 25),
    aes(x = time, y = link)) +
    geom_raster(aes(fill = norm_weight))
```

For the one case minimal example we first check what is the best embedding dimension of the data. The embedding dimension is the number of lags on a time series that the algorithm requires to better reconstruct the attractor or shadow manifold. 

```{r}
# check embedding dimension

#### make dataframe suitable for CCM
df2 <- filter(df1, time < 30, n_obs > 25) %>%
    select(time, link, norm_weight) %>%
    spread(link, norm_weight) %>%
    as.data.frame()

simplex_out <- simplex(as.data.frame(df2)[c(1,3)])
plot(simplex_out$E, simplex_out$rho, type = "l", xlab = "Embedding Dimension (E)", ylab = "Forecast Skill (rho)")
```
The forecast skill or rho [`r expression(rho)`] is literally a Persson correlation coefficient, but it is calculated between the neighbourhood of points in the manifold instead of the time series itself. Rho is then a proxy of how much information a time series contain about another time series. For now we are only using one, and the right embedding dimension is the number of delays that maximizes such forecasting capacity, in our case $1$.

Next we check for the forecasting horizon, this is how far in the future we can trust the forecasting skill of rho. For the Chile time series we see that there is a peak in forecasting skill at 6 months, followed by a year cycle and lower peaks that I believe correspond to seasonal dynamics.

```{r}
## check for forecasting horizon
simplex_output <- simplex(as.data.frame(df2)[c(1,3)], E = 1, tp = 1:12)
plot(simplex_output$tp, simplex_output$rho, type = "l", xlab = "Time to Prediction (tp)", ylab = "Forecast Skill (rho)")
```

Last we check if the dynamics are truly non-linear. This is done by using the s-map algorithm. What I understood from my CCM readings is that the parameter $\theta$ determines how non-linearity is present on the time series. If rho were in its maximum with $\theta = 0$, then the time series would be result of a linear process. A hallmark of non-linearity is a sharp increase in $\rho$ as $\theta$ increase followed by a decline at a non-zero value of $\theta$, as we observe here:

```{r}
## Identify non-linearity
smap_output <- s_map(as.data.frame(df2)[c(1,3)], E = 1)
plot(smap_output$theta, smap_output$rho, type = "l", xlab = "Nonlinearity (theta)", ylab = "Forecast Skill (rho)")
```

The last step on the process is the convergent cross mapping. What this procedure does is using the same `simplex` and `s-map` algorithms as before but instead of applying it to one time series it works with two time series. It tries to identify if there is information gained (increases in $\rho$ or our forecasting skill) from the dynamics of one time series by doing the embedding in another. If $\rho$ is zero, then the two processes are not causally related in time, but if $\rho$ is higher than zero then one time series have information from another time series. Continuing with our minimal example of 3 countires, here I test whether the trade flow between Netherlands and USA contains information of the trade flow between Chile and USA, and viceversa.

```{r}
## Convergent cross mapping:
chl_xmap_nld <- ccm(as.data.frame(df2), E=1, lib_column = "CHL_USA", target_column="NLD_USA", lib_sizes = seq(3,29, by =2), random_libs=FALSE)

nld_xmap_chl <- ccm(as.data.frame(df2), E=1, target_column = "CHL_USA", lib_column="NLD_USA", lib_sizes = seq(3,29, by =2), random_libs=FALSE)


chl_xmap_ndl_means <- data.frame(ccm_means(chl_xmap_nld), sd.rho = with(chl_xmap_nld, tapply(rho, lib_size, sd)))

ndl_xmap_chl_means <- data.frame(ccm_means(nld_xmap_chl), sd.rho = with(nld_xmap_chl, tapply(rho, lib_size, sd)))
```

The results are summarized in the following figure. In general the forecasting skill $\rho$ should increase as the library size increases. The library size is the portion of the time series used to do the forecasting. Currently I'm only using 29 months and already in the last month there is lots of missing values, that's why the confidence interval does not get calculated on the last month. However, a positive $\rho$ that does not cross the zero line on it's confidence internval already tells us that the salmon trade flow from Chile to USA contains information that is reflected on the flow from Netherland to USA, while the opposite is not true. This means that one can poorly forecast the flow of Netherlands to USA using information of what is trade with Chile, I said poorly because it explain less than 20% (I'm hesitant to say here variability but is something like that). 

```{r}
df3 <- rbind(chl_xmap_ndl_means, ndl_xmap_chl_means) %>%
  unite(xmap, lib_column, target_column, sep = " <- ")

ggplot(df3, aes(lib_size, rho, group = xmap)) +
  geom_ribbon(aes(ymin = rho - sd.rho, ymax = rho + sd.rho, fill = xmap), alpha = 0.3) +
  geom_hline(yintercept = 0, color = "black", linetype = 2) +
  geom_line(aes(color = xmap)) +
  labs(x = "Library size", y = "Forecasting skill") + # expression(rho)
  theme_light(base_size=10) + theme(legend.position = "bottom")
```

I did explore the same excercise for the 16 links of incoming trade for the USA (where we have more or less complete data). First we check again for embedding dimensions, prediction decay and nonlinearity test. 

```{r echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
## Explore the embedding dimension for all time series of the ego-network on USA
out <- list()
for (i in 2:dim(df2)[2]){
    out[[i-1]] <- df2[c(1,i)] %>%
      simplex()
}

bestE <- out %>% map_dbl(~ .$E[which.max(.$rho)])

## Prediction decay
prediction_decay <- list()
for (i in 2:dim(df2)[2]){
    prediction_decay[[i-1]] <- df2[c(1,i)] %>%
      simplex(., E = bestE[i-1], tp = 1:10)
}

## Nonlinearity test
nonlinearity <- list()
for (i in 2:dim(df2)[2]){
    nonlinearity[[i-1]] <- df2[c(1,i)] %>%
      s_map(., E = bestE[i-1])
}

## Plots
source('~/Dropbox/Code/multiplot.R')
g <- out %>% map( .,
    function(x) {
        ggplot(data = x, aes(E,rho)) +
        geom_line() +
        theme_light(base_size = 7)
        })
p_decay <- prediction_decay %>% map( .,
    function(x) {
        ggplot(data = x, aes(tp, rho)) +
        geom_line() +
        theme_light(base_size=7)
        })

non_linear <- nonlinearity %>% map(.,
    function(x){
        ggplot(data = x, aes(theta,rho)) +
        geom_line() +
        theme_light(base_size = 7)
        })
layout <- matrix(1:16, ncol = 4, nrow = 4, byrow = T)
```

```{r caption = "Embedding dimensions"}
multiplot(plotlist = g, layout = layout)
```

```{r caption = "Prediction decay"}
multiplot(plotlist = p_decay, layout = layout)
```

```{r caption = "Nonlinearity"}
multiplot(plotlist = non_linear, layout = layout)
```

Note that time series with lots of zeroes or missing values don't reach convergence. Nevertheless one can obtain an estimate of the forecasting skill for each pairwise combination of links:

```{r}
### Cross map matrix: following Hao Ye tutorial
ncol <- dim(df2)[2]-1
M_rho <- array(NA,dim=c(ncol,ncol),
  dimnames=list(colnames(df2[2:17]),colnames(df2[2:17])))

for (i in 1:ncol){
    for (j in 1:ncol){
        if (i!=j){
            out_temp <- ccm(df2,E=1,lib_column=1+i,target_column=1+j,
                            lib_sizes = dim(df2)[1],replace=FALSE, silent = TRUE)
            M_rho[i,j] <- out_temp$rho
            }
        }
    }
dfM_rho <- as_tibble(M_rho)
dfM_rho$i <- colnames(dfM_rho)
dfM_rho <- dfM_rho %>%
  select(17, 1:16) %>%
  gather(key = "j", value = "rho", 2:17)

ggplot(dfM_rho, aes(i,j, fill = rho)) +
  geom_raster() +
  scale_fill_gradient2(
      low = scales::muted("red"), mid = "white",
      high = scales::muted("blue"), midpoint = 0, space = "Lab",
      na.value = "grey50", guide = "colourbar") + # name = expression(rho)
  xlab("") + ylab("") + theme_light(base_size=6) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5 ))

```

And for comparison a correlation matrix:

```{r}
## Correlation matrix: from tutorial code
M_corr <- array(NA,dim=c(ncol,ncol),
  dimnames=list(colnames(df2[2:17]),colnames(df2[2:17])))

for (i in 1:ncol){
    for (j in 1:ncol){
        if (i!=j){
            cf_temp <- ccf(x=df2[,1+i], y=df2[,1+j], type = "correlation", lag.max = 6, plot = FALSE, na.action = na.pass)$acf
            M_corr[i,j] <- max(abs(cf_temp))
            }
        }
}

df_corr <- as_tibble(M_corr)
df_corr$i <- colnames(df_corr)
df_corr <- df_corr %>%
  select(17, 1:16) %>%
  gather(key = "j", value = "rho", 2:17)

ggplot(df_corr, aes(i,j, fill = rho)) +
geom_raster() +
scale_fill_gradient2(
    low = scales::muted("red"), mid = "white",
    high = scales::muted("blue"), midpoint = 0.5, space = "Lab",
    na.value = "grey50", guide = "colourbar") + # name = expression(rho)
xlab("") + ylab("") + theme_light(base_size=6) +
theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5 ))
```

Note that one can identify pairs of link where the forecasting skill is positive and the correlation is rather weak. That's a good sign of getting into causality links. If $\rho$ and correlation were high in both directions of the comparison, one could get issues with synchrony, but I don't think it's the case here. Now to the full network.

## Salmon trade network.

The remaining question is: does it work for the full network? To answer this question I use the full dataset but filter for trade values larger than 1 in the log scale or 2.7 tons. This is to avoid `NA` or `NaN` values when I normalize the time series, to avoid division by zero. The analysis is also restricted to timeseries that have at least 20 observations, that's 2/3 of our time horizon. Visualizing it is a bit pointless, it contains 529 normalized time series of 29 months each.

```{r}

df4 <- dat %>%
  filter(log_weight > 1) %>%
  select(Period, Exporter = Exporter.ISO3c, Importer = Importer.ISO3c,  log_weight) %>%
  unite(link, Exporter, Importer, remove = TRUE) %>%
  left_join(time_points) %>%
  group_by(link) %>%
  mutate(norm_weight = (log_weight - mean(log_weight)) / sd(log_weight),
    n_obs = n()) %>%
  filter(n_obs > 20) # at least 2/3 of time series observed.


#### transform to data frame suitable for ccm.
df4 <- filter(df4, time < 30) %>%
    select(time, link, norm_weight) %>%
    spread(link, norm_weight) %>%
    as.data.frame()

```

However, the logic of the analysis is the same as before: first we find the right embedding for each time series, then the prediction decay and non-linearity test.

```{r cache = TRUE, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
## embedding
emb <- list()
for (i in 2:dim(df4)[2]){
    emb[[i-1]] <- df4[c(1,i)] %>%
      simplex()
    emb[[i-1]]$link <- colnames(df4)[i]
}
# best embedding dimension:
bestE <- emb %>% map_dbl(~ .$E[which.max(.$rho)])

## Prediction decay
prediction_decay <- list()
for (i in 2:dim(df4)[2]){
    prediction_decay[[i-1]] <- df4[c(1,i)] %>%
      simplex(., E = bestE[i-1], tp = 1:10)
    prediction_decay[[i-1]]$link <- colnames(df4)[i]
}

## Nonlinearity test
nonlinearity <- list()
for (i in 2:dim(df4)[2]){
    nonlinearity[[i-1]] <- df4[c(1,i)] %>%
      s_map(., E = bestE[i-1])
    nonlinearity[[i-1]]$link <- colnames(df4)[i]
}
```

The resulting plot for all time series looks as follows:

```{r}
## Plots: I don't need a plot per variable anymore.
g <- emb %>%
  bind_rows() %>%
    ggplot(aes(E,rho, group = link)) +
        geom_line(alpha = 0.2) +
        labs(x = "Embedding", y = expression(rho), title = "Embedding dimension") +
        theme_light(base_size = 7)

p_decay <- prediction_decay %>% bind_rows() %>%
    ggplot(aes(tp, rho, group = link)) +
        geom_line(alpha = 0.2) +
        labs(x = "Prediction horizon", y = expression(rho), title = "Prediction decay") +
        theme_light(base_size=7)


non_linear <- nonlinearity %>% bind_rows() %>%
    ggplot(aes(theta,rho, group = link)) +
        geom_line(alpha = 0.2) +
        labs(x = expression(theta), y = expression(rho), title = "Non-linearity") +
        theme_light(base_size = 7)


# here just change the name of the plot list
layout <- matrix(1:3, ncol = 3, nrow = 1, byrow = T)
multiplot(plotlist = list(g, p_decay, non_linear), layout = layout)
```

```{r}
# ### Cross map matrix for the network
# ncol <- dim(df4)[2]-1
# M_rho <- array(NA,dim=c(ncol,ncol),
#   dimnames=list(colnames(df4[2:530]),colnames(df4[2:530])))
# 
# # J171004: Note my modification in the embedding argument. On the previous exercise I used a constant embedding, but bestE already have the best embedding for each time series. On the ccm help it says: "ccm uses time delay embedding on one time series to generate an attractor reconstruction, and then applies the simplex projection algorithm to estimate concurrent values of another time series". Therefore I assume here that the embedding required correspond to the lib_column, in this case i.
# 
# for (i in 1:ncol){
#     for (j in 1:ncol){
#         if (i!=j){
#             out_temp <- ccm(df4, E= bestE[i],
#                 lib_column=1+i, target_column=1+j ,
#                 lib_sizes = dim(df4)[1],replace=FALSE, silent = TRUE)
#             M_rho[i,j] <- out_temp$rho
#             }
#         }
#     }
# dfM_rho <- as_tibble(M_rho)
# dfM_rho$i <- colnames(dfM_rho)
# dfM_rho <- dfM_rho %>%
#   select(530, 1:529) %>%
#   gather(key = "j", value = "rho", 2:530)

load('~/Documents/Projects/Salmon_Jessica/data/salmon_ccm.Rdata')

```

The CCM for the full networks takes quite a bit of time to compute. Part of the issue is that currently it is testing all pair wise combinations of links regardless whether they are next to each other or not. I like the approach because it let us test for transitivity, this is that A -> B -> C implies that A -> C. By looking how far that signal dissipates we can answer the question how far in the network a potential shock would spread depending of the country where it starts. The monster looks as follows:

```{r}
ggplot(dfM_rho %>%
    filter(rho > 0)
    , aes(i,j, fill = rho)) +
  geom_raster() +
  scale_fill_gradient2(
      low = scales::muted("red"), mid = "white",
      high = scales::muted("blue"), midpoint = 0, space = "Lab",
      na.value = "grey50", guide = "colourbar") + # name = expression(rho)
  xlab("") + ylab("") + theme_light(base_size=6) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5 ))
```

And here is where I'm stuck. Each row represent how a trade flow between two countries can predict or not the dynamics of flow represented in each column. How does one translate that back to a network of countries trading salmon? One option would be average out the $\rho$ coefficient of each row and use it as attribute in the network, but that opens the posibility for canceling out causality effects. For example, salmon traded from Chile to USA can have an effect on salmon traded between Netherlands and USA, but not between Spain and USA. If we average we can risk canceling out effects. Do you have any ideas of how to translate the results back to the original network? Part of the problem is that the time series represent links not nodes. So the resulting matrix is really a "network" or relationships between links, and each link involves the bilateral trade of two countries.

However, on the line of averaging per rows, this is what I found. I plotted the mean $\rho$ per row, calculated the variance and plotted as confidence intervals. I also calculated the ratio of cases where $\rho$ is positive to account for that potential cancelling effect. Here is the result:

```{r}
dfM_rho %>%
  group_by(i) %>%
  summarize(mean_rho = mean(rho, na.rm = TRUE),
  var_rho = var(rho, na.rm = TRUE),
  ratio_cases = sum(rho > 0, na.rm = TRUE) / 529 ) %>%
  ggplot(aes(x = mean_rho, y = ratio_cases)) +
  geom_errorbarh(aes(xmin = mean_rho - var_rho, 
      xmax = mean_rho + var_rho, y = ratio_cases), color = "grey84", alpha = 0.8) + geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  geom_hline(yintercept = 0.5, color = "orange", linetype = 2) +
  geom_vline(xintercept = 0, color = "orange", linetype = 2) +
  coord_flip() +
  theme_light()

```

Looks nice, the relationship is almost linear although sublinear on the extremes. The take home message is that the higher the $\rho$ also the higher the likelihood that it affect many coupled links. What is left to check is whether such points with positive forecasting skill are true nodes on the network or spurious correlations. It could be the case if two countries have high forecasting skill not because they trade together or with similar partners, but because they trade timeseries are incluenced by the same external forcing. For example think of Peru and Chile, both countries and their trade dynamics might be influenced by ENSO effects. I'm still working on bringing these results to the network. 

Any ideas are more than welcome!!!

